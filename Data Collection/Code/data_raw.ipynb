{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.select import Select\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_articles(journal, start, end, times):\n",
    "    \n",
    "    email = 'xianlin.ding@sciencespo.fr'\n",
    "    password = '099115Keep12?'\n",
    "    path = os.getcwd()\n",
    "    path = re.sub('Code','Result',path)\n",
    "    \n",
    "    # go into the Eruopresse website\n",
    "    url = \"https://acces-distant.sciencespo.fr/fork?https://nouveau.europresse.com/access/ip/default.aspx?un=politique2T_1\"\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    driver.get(url)\n",
    "    driver.find_element(By.ID, 'username').send_keys(email)\n",
    "    driver.find_element(By.ID, 'password').send_keys(password)\n",
    "    time.sleep(3)\n",
    "    driver.find_element(By.CLASS_NAME, 'btn-submit').click()\n",
    "    \n",
    "    # choose the criteria of searching\n",
    "    driver.find_element(By.CLASS_NAME, 'lnk-text').click()\n",
    "    time.sleep(3)\n",
    "    driver.find_element(By.ID, 'specific-sources-rd').click()\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # choose the journal\n",
    "    try:\n",
    "        driver.find_element(By.ID, journal).click()\n",
    "    except:\n",
    "        time.sleep(20)\n",
    "        driver.find_element(By.ID, journal).click()\n",
    "    \n",
    "    # key words : climat and climatique\n",
    "    driver.find_element(By.XPATH, '//textarea').send_keys('climat') \n",
    "    driver.find_elements(By.CLASS_NAME, 'criteria-oper-lbl')[1].click()\n",
    "    Select(driver.find_element(By.NAME, 'CriteriaKeys[0].Key')).select_by_value('TEXT')\n",
    "    driver.find_element(By.NAME, 'CriteriaKeys[0].Text').send_keys('climatique')\n",
    "    \n",
    "    # choose the searching period\n",
    "    Select(driver.find_element(By.ID, 'DateFilter_DateRange')).select_by_value('10')\n",
    "\n",
    "    Select(driver.find_elements(By.CLASS_NAME, 'year')[1]).select_by_value(start[0])\n",
    "    Select(driver.find_elements(By.CLASS_NAME, 'year')[2]).select_by_value(end[0])\n",
    "\n",
    "    Select(driver.find_elements(By.CLASS_NAME, 'month')[1]).select_by_value(start[1])\n",
    "    Select(driver.find_elements(By.CLASS_NAME, 'month')[2]).select_by_value(end[1])\n",
    "\n",
    "    Select(driver.find_elements(By.CLASS_NAME, 'day')[1]).select_by_value(start[2])\n",
    "    Select(driver.find_elements(By.CLASS_NAME, 'day')[2]).select_by_value(end[2])\n",
    "    \n",
    "    driver.find_element(By.ID, 'btnSearch').click()\n",
    "    \n",
    "    # scroll page\n",
    "    for i in range(times):\n",
    "        temp_height=0\n",
    "        while True:\n",
    "            driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "            check_height = driver.execute_script(\"return document.documentElement.scrollTop || window.pageYOffset || document.body.scrollTop;\")\n",
    "            if check_height==temp_height:\n",
    "                break\n",
    "            temp_height=check_height\n",
    "        \n",
    "        time.sleep(5)\n",
    "        if (i+1) % 5 == 0:\n",
    "            print('scrolling ', i+1, ' times' )\n",
    "        \n",
    "    # get web contents\n",
    "    web_content = driver.page_source\n",
    "    obj = BeautifulSoup(web_content,'lxml').body\n",
    "    obj = obj.aside.ul\n",
    "    obj = obj.find_all(name='div', attrs={\"class\":\"docListItem msDocItem\"})\n",
    "    \n",
    "    # start to build data\n",
    "    info = pd.DataFrame(columns = ['date', 'publication', \n",
    "                                   'words', 'author',\n",
    "                                   'title', 'intro'])\n",
    "    \n",
    "    i=0\n",
    "    for obj_i in obj:\n",
    "        \n",
    "        obj_i = obj_i.div.next_sibling.next_sibling\n",
    "        \n",
    "        # publication\n",
    "        obj_i_1 = obj_i.div\n",
    "        obj_i_1 = obj_i_1.find_all(name='span', attrs={\"class\":\"source-name\"})[0]\n",
    "        obj_i_1 = str(obj_i_1)\n",
    "        publication_i = re.findall(r\"<span class=\\\"source-name\\\">(.+?)</span>\", obj_i_1)[0]\n",
    "        \n",
    "        # title\n",
    "        obj_i_2 = obj_i.div.next_sibling.next_sibling\n",
    "        title_i = obj_i_2.div.a\n",
    "        title_i = str(title_i)\n",
    "        title_i = re.findall(r\">(.+?)</a>\", title_i)[0]\n",
    "        title_i = re.sub('\\u200a','',title_i)\n",
    "        \n",
    "        # date and words\n",
    "        obj_i_3 = obj_i_2.div.next_sibling.next_sibling \n",
    "        \n",
    "        date_words_i = obj_i_3.div\n",
    "        date_words_i = str(date_words_i)\n",
    "        date_i = re.findall(r\"<span class=\\\"details\\\">(.+?)<span class=\", date_words_i)[0]\n",
    "        date_i = date_i.replace(' ', '')\n",
    "        \n",
    "        words_i = re.findall(r\"</span>(.+?)</span>\", date_words_i)[0]\n",
    "        words_i = words_i.replace(' ', '')\n",
    "        \n",
    "        # authors and introduction\n",
    "        obj_i_4 = obj_i_3.div.next_sibling.next_sibling\n",
    "        obj_i_4 = str(obj_i_4)\n",
    "        \n",
    "        try:\n",
    "            obj_i_4 = obj_i_4.split('<span class=\"doclist-author\">')[1]\n",
    "            author_i = obj_i_4.split('</span>')[0]\n",
    "            intro_i = obj_i_4.split('</span>')[1]\n",
    "        \n",
    "        except:\n",
    "            author_i = None\n",
    "            intro_i = obj_i_4.split('</span> </span>')[1]\n",
    "    \n",
    "        intro_i = intro_i.split('</div>')[0]\n",
    "        intro_i = re.sub(r\"<([a-z]+)>\",'',intro_i)\n",
    "        intro_i = re.sub(r\"</([a-z]+)>\",'',intro_i)\n",
    "        intro_i = re.sub('\\u200a','',intro_i)\n",
    "        intro_i = re.sub('-','',intro_i)\n",
    "        \n",
    "        info.loc[i] = [date_i, publication_i, \n",
    "                       words_i, author_i,\n",
    "                       title_i, intro_i]\n",
    "        i = i+1\n",
    "    \n",
    "    info_name = str(publication_i) + '_' + \\\n",
    "              str(start[0]) + '_' + \\\n",
    "              str(start[1]) + '_' + str(end[1])\n",
    "    \n",
    "    info.to_csv(path + '\\\\data_raw\\\\'+ info_name + '.csv')\n",
    "    \n",
    "    print('Import Data ' + info_name + 'finished.')\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_period(year):\n",
    "    \n",
    "    start_md = [['1','1'],['5','1'],['9','1']]\n",
    "    end_md = [['4','30'], ['8','31'], ['12','31']]\n",
    "    \n",
    "    start = []\n",
    "    for i in year:    \n",
    "        for j in start_md:\n",
    "            start_ij = []\n",
    "            start_ij.append(i)\n",
    "            start_ij = start_ij + j\n",
    "            start.append(start_ij)\n",
    "            \n",
    "    end = []\n",
    "    for i in year:    \n",
    "        for j in end_md:\n",
    "            end_ij = []\n",
    "            end_ij.append(i)\n",
    "            end_ij = end_ij + j\n",
    "            end.append(end_ij)\n",
    "            \n",
    "    return start,end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid missing data during collection, \n",
    "# We split it into several parts \n",
    "# by journals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-77-3b72dbe67953>:10: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scrolling  5  times\n",
      "scrolling  10  times\n",
      "scrolling  15  times\n",
      "scrolling  20  times\n",
      "scrolling  25  times\n",
      "scrolling  30  times\n",
      "scrolling  35  times\n",
      "Import Data Le Monde_2018_1_4finished.\n",
      "scrolling  5  times\n",
      "scrolling  10  times\n",
      "scrolling  15  times\n",
      "scrolling  20  times\n",
      "scrolling  25  times\n",
      "scrolling  30  times\n",
      "scrolling  35  times\n",
      "Import Data Le Monde_2018_5_8finished.\n",
      "scrolling  5  times\n",
      "scrolling  10  times\n",
      "scrolling  15  times\n",
      "scrolling  20  times\n",
      "scrolling  25  times\n",
      "scrolling  30  times\n",
      "scrolling  35  times\n",
      "Import Data Le Monde_2018_9_12finished.\n",
      "scrolling  5  times\n",
      "scrolling  10  times\n",
      "scrolling  15  times\n",
      "scrolling  20  times\n",
      "scrolling  25  times\n",
      "scrolling  30  times\n",
      "scrolling  35  times\n",
      "Import Data Le Monde_2019_1_4finished.\n",
      "scrolling  5  times\n",
      "scrolling  10  times\n",
      "scrolling  15  times\n",
      "scrolling  20  times\n",
      "scrolling  25  times\n",
      "scrolling  30  times\n",
      "scrolling  35  times\n",
      "Import Data Le Monde_2019_5_8finished.\n",
      "scrolling  5  times\n",
      "scrolling  10  times\n",
      "scrolling  15  times\n",
      "scrolling  20  times\n",
      "scrolling  25  times\n",
      "scrolling  30  times\n",
      "scrolling  35  times\n",
      "Import Data Le Monde_2019_9_12finished.\n",
      "scrolling  5  times\n",
      "scrolling  10  times\n",
      "scrolling  15  times\n",
      "scrolling  20  times\n",
      "scrolling  25  times\n",
      "scrolling  30  times\n",
      "scrolling  35  times\n",
      "Import Data Le Monde_2020_1_4finished.\n",
      "scrolling  5  times\n",
      "scrolling  10  times\n",
      "scrolling  15  times\n",
      "scrolling  20  times\n",
      "scrolling  25  times\n",
      "scrolling  30  times\n",
      "scrolling  35  times\n",
      "Import Data Le Monde_2020_5_8finished.\n",
      "scrolling  5  times\n",
      "scrolling  10  times\n",
      "scrolling  15  times\n",
      "scrolling  20  times\n",
      "scrolling  25  times\n",
      "scrolling  30  times\n",
      "scrolling  35  times\n",
      "Import Data Le Monde_2020_9_12finished.\n",
      "scrolling  5  times\n",
      "scrolling  10  times\n",
      "scrolling  15  times\n",
      "scrolling  20  times\n",
      "scrolling  25  times\n",
      "scrolling  30  times\n",
      "scrolling  35  times\n",
      "Import Data Le Monde_2021_1_4finished.\n",
      "scrolling  5  times\n",
      "scrolling  10  times\n",
      "scrolling  15  times\n",
      "scrolling  20  times\n",
      "scrolling  25  times\n",
      "scrolling  30  times\n",
      "scrolling  35  times\n",
      "Import Data Le Monde_2021_5_8finished.\n",
      "scrolling  5  times\n",
      "scrolling  10  times\n",
      "scrolling  15  times\n",
      "scrolling  20  times\n",
      "scrolling  25  times\n",
      "scrolling  30  times\n",
      "scrolling  35  times\n",
      "Import Data Le Monde_2021_9_12finished.\n",
      "scrolling  5  times\n",
      "scrolling  10  times\n",
      "scrolling  15  times\n",
      "scrolling  20  times\n",
      "scrolling  25  times\n",
      "scrolling  30  times\n",
      "scrolling  35  times\n",
      "Import Data Le Monde_2022_1_4finished.\n",
      "scrolling  5  times\n",
      "scrolling  10  times\n",
      "scrolling  15  times\n",
      "scrolling  20  times\n",
      "scrolling  25  times\n",
      "scrolling  30  times\n",
      "scrolling  35  times\n",
      "Import Data Le Monde_2022_5_8finished.\n",
      "scrolling  5  times\n",
      "scrolling  10  times\n",
      "scrolling  15  times\n",
      "scrolling  20  times\n",
      "scrolling  25  times\n",
      "scrolling  30  times\n",
      "scrolling  35  times\n",
      "Import Data Le Monde_2022_9_12finished.\n"
     ]
    }
   ],
   "source": [
    "## Le monde : journal_list = 'sf_247'\n",
    "\n",
    "## last five years \n",
    "## the first one is an example\n",
    "year_list = list(map(lambda x:str(x),list(range(2018,2023))))\n",
    "start_list, end_list = generate_period(year_list)\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    for j in range(3):\n",
    "        \n",
    "        journal = 'sf_247'\n",
    "        start = start_list[i*3 + j]\n",
    "        end = end_list[i*3 + j]\n",
    "        times = 35\n",
    "        \n",
    "        scrape_articles(journal, start, end, times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-77-3b72dbe67953>:10: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scrolling  5  times\n",
      "scrolling  10  times\n",
      "scrolling  15  times\n",
      "scrolling  20  times\n",
      "scrolling  25  times\n",
      "scrolling  30  times\n",
      "scrolling  35  times\n",
      "Import Data Le Monde_2022_9_10finished.\n",
      "scrolling  5  times\n",
      "scrolling  10  times\n",
      "scrolling  15  times\n",
      "scrolling  20  times\n",
      "scrolling  25  times\n",
      "scrolling  30  times\n",
      "scrolling  35  times\n",
      "Import Data Le Monde_2022_11_12finished.\n"
     ]
    }
   ],
   "source": [
    "# the data for Le Monde_2022_9_12  is not fully scraped (more than 1000 articles)\n",
    "# re-do it\n",
    "scrape_articles(journal, ['2022', '9', '1'], ['2022', '10', '31'], times)\n",
    "scrape_articles(journal, ['2022', '11', '1'], ['2022', '12', '31'], times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now change it into functions\n",
    "def import_data(journal_code, year_list):\n",
    "    \n",
    "    start_list, end_list = generate_period(year_list)\n",
    "    \n",
    "    for i in range(len(year_list)):\n",
    "        for j in range(3):\n",
    "            \n",
    "            journal = journal_code\n",
    "            start = start_list[i*3 + j]\n",
    "            end = end_list[i*3 + j]\n",
    "            times = 25\n",
    "            scrape_articles(journal, start, end, times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue import data\n",
    "## Le monde, 2013-2017\n",
    "\n",
    "journal_code = 'sf_247'\n",
    "year_list = list(map(lambda x:str(x),list(range(2013,2018))))\n",
    "\n",
    "import_data(journal_code, year_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Le Figaro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2022', '9', '1']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Les Echos\n",
    "start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Libération\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## La Croix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Le Parisien\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
